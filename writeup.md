# Project: Perception Pick & Place

[//]: # (Image References)

[image1]: ./images/model_res_1.png
[image2]: ./images/model_res_2.png
[image3]: ./images/list_1.png
[image4]: ./images/list_2.png
[image5]: ./images/list_3.png

## [Rubric](https://review.udacity.com/#!/rubrics/1067/view) Points
### Here I will consider the rubric points individually and describe how I addressed each point in my implementation.  

---
### Writeup / README

#### 1. Provide a Writeup / README that includes all the rubric points and how you addressed each one.  You can submit your writeup as markdown or pdf.  

You're reading it!

### Exercise 1, 2 and 3 pipeline implemented
#### 1. Complete Exercise 1 steps. Pipeline for filtering and RANSAC plane fitting implemented.

In this process, the following steps was included for pre-processing images the robot will see.

1 Voxel Grid Filter was included to reduce the number of points needed to be process. LEAF_SIZE was selected as 0.005 so that there were not too many points and the objects were not too sparse.

2 Passthrough Filter was included for only focusing on the area containing objects. Range 0.6 to 1.1 on z axis and range -0.5 to 0.5 on y axis was selected to contain all the objects.

3 Outlier Remover Filter was included to remove the noises. K was selected to be 100, threshold was selected to one standard deviation, so that mean distance of K neighour bigger than the standard deviation would be regarded as noise. In this configuration, most noise would be removed.

4 RANSAC Plane Segmentation was used to segment that table because the tabel is a plane in the image. RANSAC would try to model the plane with sampled points and evaluate inliers and outliers, the model with best evaluation would be used to segment the table.

5 ExtractIndices Filter was used to extract objects from the segmentation result of RANSAC, which is the outlier of the table.

#### 2. Complete Exercise 2 steps: Pipeline including clustering for segmentation implemented.  

In this process, I was trying to do clustering to find the real objects and remove the remaining noise. KMeans and DBScan was commonly used clustering algorithms, in this case, number of objects could not be given, so EuclideanClusterExtraction as a DBScan implementation was used. MinClusterSize was selected to be 200 so that fake cluster with number smaller than 200 would be removed. MaxClusterSize was selected to be 10000 because book was a big object. 

#### 2. Complete Exercise 3 Steps.  Features extracted and SVM trained.  Object recognition implemented.

In last process, I tried to find out the shape of objects and remove noise by clustering.
In this process, I was trying to recognize the objects which were found by clustering. The point cloud of every object was extracted from the cloud object which is generated by steps of Exercise-1. After generating hsv-color histograms and normal histograms features, I used the pre-trained svm model to recognize the object.

Linear svm was selected to use in this case because it's powerful enough and fast. I trained the model on eight objects, 100 samples were randomly generated for each objects. The evaluation confusion matrix were followed.

![image1] ![image2]

### Pick and Place Setup

#### 1. For all three tabletop setups (`test*.world`), perform object recognition, then read in respective pick list (`pick_list_*.yaml`). Next construct the messages that would comprise a valid `PickPlace` request output them to `.yaml` format.

I tried to preform object recognition in test1/2/3 world, with 100%, 80% and 87.5% correct result. The results were as followed.

![image3] ![image4]![image5]


In the last step in this project, I was trying to calculate the centroids of all the objects in differents worlds. I tried to get object lists needed to pick by ```objects_to_pick = rospy.get_param('/object_list')``` and dropbox position by the following code snippet.

    boxs_to_place = rospy.get_param('/dropbox')
    boxs_pos = dict()
    for box in boxs_to_place:
        boxs_pos[box['group']] = box['position']


Then for every 'object_to_pick', I tried to find it from the result of recognition process by the object label. The following code snippet was used to caculate the centroids, pick/place position, object name and arm name.

Imporvement could be done on the clustering step, because MinClusterSize was set to 200, there were only seven objects in test3 world. Also svm model could be trained on more samples, like 1000 sample for each objects.  